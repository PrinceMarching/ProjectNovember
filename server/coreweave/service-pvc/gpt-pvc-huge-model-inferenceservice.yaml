apiVersion: serving.kubeflow.org/v1alpha2
kind: InferenceService
metadata:
  name: gpt-pvc
  annotations:
    serving.kubeflow.org/gke-accelerator: Tesla_V100
# leave commented out when loading pods the first time
# try enabling it later, after they're done loading
    autoscaling.knative.dev/scaleToZeroPodRetentionPeriod: "30m"
spec:
  default:
    predictor:
      parallelism: 1
      # Target concurrency of 4 active requests to each container
      minReplicas: 0 # Allow scale to zero
      maxReplicas: 20
      tensorflow:
        storageUri: pvc://model-storage/1558M/ # The PVC and path inside the PVC to the model
        runtimeVersion: "1.15.0-gpu"
        resources:
          limits:
            cpu: 1
            memory: 12Gi
            nvidia.com/gpu: 1

    transformer:
      parallelism: 40
      # keep one transformer online at all times
      # it's cheap to run, and we don't want the extra time it takes 
      # to spin it up
      minReplicas: 1
      maxReplicas: 2
      custom:
        container:
          image: coreweave/gpt-transformer:0.9 # Docker image of the code found in transformer/
          name: user-container
          resources:
            limits:
              cpu: 3000m
              memory: 1Gi
            requests:
              cpu: 300m
              memory: 256Mi
