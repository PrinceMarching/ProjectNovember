{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Serving Warmup\n",
    "\n",
    "The warmup data is read by Tensorflow Serving on startup to prime the model before accepting user requests. This makes the first user request as fast as expected. A set of realistic inputs should be used for the warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model storage is mounted in this Pod as well, and we can put stuff in there directly from jupyter\n",
    "!ls /models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tensorflow-serving-api==1.14.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_log_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Writing to temp instead of writing to the production folder directly\n",
    "model_dir = '/models/1558M/1597347193/' \n",
    "model_name = 'gpt-pvc' # I do not know if this matters\n",
    "# Two example input texts grabbed from production requests\n",
    "# With these two examples, warmup takes 14s\n",
    "warmup_contexts = [\n",
    "    [ 200, 201, 202 ],\n",
    "    [ 300, 301, 302 ],\n",
    "    [ 400, 401, 402 ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_dir = os.path.join(model_dir, 'assets.extra')\n",
    "if not os.path.exists(assets_dir):\n",
    "    os.mkdir(assets_dir)\n",
    "\n",
    "warmup_file = os.path.join(assets_dir, 'tf_serving_warmup_requests')\n",
    "with tf.io.TFRecordWriter(warmup_file) as writer:\n",
    "    for context in warmup_contexts:\n",
    "      # Create the inference request\n",
    "      request = predict_pb2.PredictRequest()\n",
    "      request.model_spec.name = model_name\n",
    "      request.model_spec.signature_name = 'predict'\n",
    "    \n",
    "      # Should add some variability for these options as well\n",
    "      request.inputs['context'].CopyFrom(\n",
    "          tf.make_tensor_proto(context, shape=[1, len(context)]))\n",
    "\n",
    "    \n",
    "      log = prediction_log_pb2.PredictionLog(\n",
    "          predict_log=prediction_log_pb2.PredictLog(request=request))\n",
    "      writer.write(log.SerializeToString())\n",
    "\n",
    "print('Created the file \\'%s\\'' % warmup_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
