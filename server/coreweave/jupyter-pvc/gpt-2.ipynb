{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kc5cIgeEmv8o"
   },
   "source": [
    "# Exporting GPT-2\n",
    "In this notebook, we'll show how to export [OpenAI's GPT-2 text generation model](https://github.com/openai/gpt-2) for serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RAWs29lAktOK"
   },
   "source": [
    "First, we'll download the GPT-2 code repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHs3aaFaLUXq"
   },
   "outputs": [],
   "source": [
    "!apt update\n",
    "!apt install -y git\n",
    "!git clone --no-checkout https://github.com/openai/gpt-2.git\n",
    "!cd gpt-2 && git reset --hard ac5d52295f8a1c3856ea24fb239087cc1a3d1131"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we install the Python dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install requests tqdm tensorflow==1.14.* numpy==1.* boto3==1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A4al4P14nmni"
   },
   "source": [
    "Next we'll specify the model size (choose one of 124M, 355M, or 774M). 124M fits well into a 8GB GPU such as a GeForce 1070 Ti for serving, so we pick that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "3Y4bt6hkfuxY"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "MODEL_SIZE = \"124M\" \n",
    "\n",
    "if MODEL_SIZE not in {\"124M\", \"355M\", \"774M\"}:\n",
    "    print(\"\\033[91m{}\\033[00m\".format('ERROR: MODEL_SIZE must be \"124M\", \"355M\", or \"774M\"'), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6xRx0Monh_j"
   },
   "source": [
    "We can use `download_model.py` to download the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kb50Z6NjbJBN"
   },
   "outputs": [],
   "source": [
    "!python3 ./gpt-2/download_model.py $MODEL_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KkVf5FmuUMrl"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import predict_signature_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Ay7qiQFoWRn"
   },
   "source": [
    "Now we can export the model for serving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GdnYXr1IKaF0"
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(), 'gpt-2/src'))\n",
    "import model, sample\n",
    "\n",
    "def export_for_serving(\n",
    "    model_name='124M',\n",
    "    seed=None,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=0,\n",
    "    models_dir='models'\n",
    "):\n",
    "    \"\"\"\n",
    "    Export the model for TF Serving\n",
    "    :model_name=124M : String, which model to use\n",
    "    :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
    "     results\n",
    "    :length=None : Number of tokens in generated text, if None (default), is\n",
    "     determined by model hyperparameters\n",
    "    :temperature=1 : Float value controlling randomness in boltzmann\n",
    "     distribution. Lower temperature results in less random completions. As the\n",
    "     temperature approaches zero, the model will become deterministic and\n",
    "     repetitive. Higher temperature results in more random completions.\n",
    "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
    "     considered for each step (token), resulting in deterministic completions,\n",
    "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
    "     special setting meaning no restrictions. 40 generally is a good value.\n",
    "     :models_dir : path to parent folder containing model subfolders\n",
    "     (i.e. contains the <model_name> folder)\n",
    "    \"\"\"\n",
    "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "    export_dir = '/models'\n",
    "\n",
    "    hparams = model.default_hparams()\n",
    "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    if length is None:\n",
    "        length = hparams.n_ctx\n",
    "    elif length > hparams.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        output = sample.sample_sequence(\n",
    "            hparams=hparams, length=length,\n",
    "            context=context,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k\n",
    "        )\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "        export_dir=os.path.join(export_dir, model_name, str(time.time()).split('.')[0])\n",
    "        if not os.path.isdir(export_dir):\n",
    "            os.makedirs(export_dir)\n",
    "\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "        signature = predict_signature_def(inputs={'context': context},\n",
    "        outputs={'sample': output})\n",
    "\n",
    "        builder.add_meta_graph_and_variables(sess,\n",
    "                                     [tf.saved_model.SERVING],\n",
    "                                     signature_def_map={\"predict\": signature},\n",
    "                                     strip_default_attrs=True)\n",
    "        builder.save()\n",
    "\n",
    "\n",
    "export_for_serving(top_k=40, length=20, model_name=MODEL_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IIMVPhe2qkU4"
   },
   "source": [
    "Verify that the model is written to the shared filesystem on `/models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdN8MtZxsO9V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/models/:\r\n",
      "total 1\r\n",
      "drwxr-xr-x 4 root root 499340732 Feb 10 04:23 124M\r\n",
      "\r\n",
      "/models/124M:\r\n",
      "total 1\r\n",
      "drwxr-xr-x 3 root root 499340732 Feb 10 04:20 1581308418\r\n",
      "drwxr-xr-x 3 root root 499340732 Feb 10 04:23 1581308605\r\n",
      "\r\n",
      "/models/124M/1581308418:\r\n",
      "total 1540\r\n",
      "-rw-r--r-- 1 root root   1576285 Feb 10 04:20 saved_model.pb\r\n",
      "drwxr-xr-x 2 root root 497764447 Feb 10 04:20 variables\r\n",
      "\r\n",
      "/models/124M/1581308418/variables:\r\n",
      "total 486099\r\n",
      "-rw-r--r-- 1 root root 497759232 Feb 10 04:20 variables.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 root root      5215 Feb 10 04:20 variables.index\r\n",
      "\r\n",
      "/models/124M/1581308605:\r\n",
      "total 1540\r\n",
      "-rw-r--r-- 1 root root   1576285 Feb 10 04:23 saved_model.pb\r\n",
      "drwxr-xr-x 2 root root 497764447 Feb 10 04:23 variables\r\n",
      "\r\n",
      "/models/124M/1581308605/variables:\r\n",
      "total 486099\r\n",
      "-rw-r--r-- 1 root root 497759232 Feb 10 04:23 variables.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 root root      5215 Feb 10 04:23 variables.index\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lR /models/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We can now continue to deploy the Inference Service.\n",
    "\n",
    "*This notebook is derived from original work by [Cortex](https://www.cortex.dev)*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "gpt-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
