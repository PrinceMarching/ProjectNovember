apiVersion: apps/v1
kind: Deployment
metadata:
  name: tensorflow-jupyter
spec:
  strategy:
    type: Recreate
  # Replicas controls the number of instances of the Pod to maintain running at all times
  # 0 means we only spin one up when we get a request, and scale down to 0 after 30m
  # of no requests
  replicas: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: tensorflow-jupyter
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tensorflow-jupyter
    spec:
      containers:
        - name: tf
          image: tensorflow/tensorflow:1.15.0-gpu-py3-jupyter

          ports:
            - name: notebook
              containerPort: 8888
              protocol: TCP

          readinessProbe:
            tcpSocket:
              port: notebook
            initialDelaySeconds: 5
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /
              port: notebook
            initialDelaySeconds: 15
            periodSeconds: 15
            failureThreshold: 3
            timeoutSeconds: 10

          volumeMounts:
            - name: storage
              mountPath: /tf/notebooks
            - name: model-storage
              mountPath: /models

          resources:
            limits:
              # limiting to 3 cpus picks cheaper inference, PCIE V100 nodes
              cpu: 3
              memory: 16Gi
              # GPUs can only be allocated as a limit, which both reserves and limits the number of GPUs the Pod will have access to
              # Making individual Pods resource light is advantageous for bin-packing. In the case of Jupyter, we stick to two GPUs for
              # demonstration purposes
              nvidia.com/gpu: 1

      # Node affinity can be used to require / prefer the Pods to be scheduled on a node with a specific hardware type
      # No affinity allows scheduling on all hardware types that can fulfill the resource request.
      # In this example, without affinity, any NVIDIA GPU would be allowed to run the Pod.
      # Read more about affinity at: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
      affinity:
        nodeAffinity:
          # This will REQUIRE the Pod to be run on a system with a Tesla v100 GPU
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: gpu.nvidia.com/model
                operator: In
                values:
                  - "Tesla_V100"

          # As ML testing doesn't require a lot of network throughput, we try to play nice and only schedule
          # the Pod on systems with only 1G network connections. We also desire decent CPUs. This is a preference, not a requirement.
          # If systems with i5 / i9 / Xeon CPUs and/or 1G ethernet are not available to fulfill the requested resources, the Pods
          # will be scheduled on higher end systems.
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 10
              preference:
                matchExpressions:
                - key: cpu.atlantic.cloud/family
                  operator: In
                  values:
                    - i5
                    - i7
                    - i9
                    - xeon
                    - ryzen
          
            - weight: 1
              preference:
                matchExpressions:
                - key: ethernet.atlantic.cloud/speed
                  operator: In
                  values:
                    - 1G
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: jupyter-pv-claim
        - name: model-storage
          persistentVolumeClaim:
            claimName: model-storage
      restartPolicy: Always
